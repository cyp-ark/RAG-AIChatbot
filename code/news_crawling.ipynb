{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27b26a95a7444e6a4c0e880b5c74b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엑셀 파일 저장 완료: ./crawl/news_20231201.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231202.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231203.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231204.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231205.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231206.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231207.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231208.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231209.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231210.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231211.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231212.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231213.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231214.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231215.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231216.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231217.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231218.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231219.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231220.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231221.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231222.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231223.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231224.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231225.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231226.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231227.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231228.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231229.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231230.xlsx\n",
      "엑셀 파일 저장 완료: ./crawl/news_20231231.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from openpyxl import *\n",
    "\n",
    "# ChromeDriver 경로 설정 (macOS 사용자의 경우 절대 경로로 설정 필요)\n",
    "chromedriver_path = '/opt/homebrew/bin/chromedriver'  # 실제 chromedriver 경로로 변경\n",
    "\n",
    "# Selenium 4.x에서 Service 객체 사용\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # 메모리 제한 해제\n",
    "chrome_options.add_argument(\"--no-sandbox\")            # 샌드박스 모드 비활성화\n",
    "chrome_options.add_argument(\"--headless\")              # 브라우저를 헤드리스 모드로 실행 (필요에 따라 삭제)\n",
    "\n",
    "# 네이버 뉴스 페이지\n",
    "link = 'https://news.naver.com/breakingnews/section/101/259?date='\n",
    "\n",
    "# 시작 날짜와 종료 날짜 정의\n",
    "start_date = datetime(2023, 12, 1)\n",
    "end_date = datetime(2023, 12, 31)  # 테스트 범위를 줄여 설정 (원래: 2024, 11, 30)\n",
    "\n",
    "# 날짜 리스트 생성\n",
    "date_list = [(start_date + timedelta(days=x)).strftime('%Y%m%d') for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# 날짜별 크롤링\n",
    "for date in tqdm(date_list):\n",
    "    retries = 3  # 재시도 횟수\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            # 새로운 드라이버 생성\n",
    "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "            # 날짜별 링크 이동\n",
    "            main_link = link + date\n",
    "            driver.get(main_link)\n",
    "            time.sleep(3)\n",
    "\n",
    "            # 더보기 버튼 클릭\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, 'section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON'))\n",
    "                )\n",
    "                while True:\n",
    "                    try:\n",
    "                        more_button = driver.find_element(By.CLASS_NAME, 'section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON')\n",
    "                        more_button.click()\n",
    "                        time.sleep(3)\n",
    "                    except Exception:\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(\"More button not found or no more articles to load:\", e)\n",
    "\n",
    "            # 기사 추출\n",
    "            articles = driver.find_elements(By.CLASS_NAME, 'sa_text_title')\n",
    "            data = [[i + 1, article.text.strip(), article.get_attribute('href')] for i, article in enumerate(articles)]\n",
    "\n",
    "            # DataFrame 생성 및 저장\n",
    "            Main_link = pd.DataFrame(data, columns=['number', 'title', 'link'])\n",
    "            excel_name = f'./crawl/news_{date}.xlsx'\n",
    "            Main_link.to_excel(excel_name, index=False)\n",
    "            print(f'엑셀 파일 저장 완료: {excel_name}')\n",
    "\n",
    "            break  # 작업 성공 시 루프 종료\n",
    "        except Exception as e:\n",
    "            print(f\"Retrying date {date}, attempts left: {retries-1}, Error: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)  # 재시도 전 대기\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    if retries == 0:\n",
    "        print(f\"Failed to process date {date} after multiple retries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 날짜와 종료 날짜 정의\n",
    "start_date = datetime(2023, 12, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "# 날짜 리스트 생성\n",
    "date_list = [(start_date + timedelta(days=x)).strftime('%Y%m%d') for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "for date in tqdm(date_list):\n",
    "    # 첫 번째 코드에서 지정한 뉴스의 링크들이 담긴 파일\n",
    "    link = pd.read_excel(f'./crawl/news_{date}.xlsx')\n",
    "    # 엑셀 파일의 빈칸이 있기에 최종 결과파일을 별도로 생성\n",
    "    excel_name = f'./newsdata/news_detail_{date}.xlsx'\n",
    "    Main_link = list(link['link'])\n",
    "\n",
    "    # number: 기사 순서, title: 기사 제목, information: 본문 내용, link: 기사의 링크\n",
    "    Information = pd.DataFrame({'number': [], 'title': [], 'information': [], 'link': []})\n",
    "    # 본문 내용을 추가하기 전이기 때문에 미리 나머지 내용을 담아둠\n",
    "    Information['number'] = link['number']\n",
    "    Information['title'] = link['title']\n",
    "    Information['link'] = link['link']\n",
    "    information = []\n",
    "\n",
    "    for main_link in Main_link:\n",
    "        # 기사가 전체적으로 2개의 구조를 가지고 있음 (게임/리뷰 카테고리에 한하여)\n",
    "        # 하나의 구조를 기준으로 삼고, 해당 부분에서 오류가 발생하면 다음 구조의 기사로 판단\n",
    "        try:\n",
    "            response = requests.get(main_link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            if response.status_code == 200:\n",
    "                html = response.content\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                # 기사의 본문 내용만 담고 있는 부분\n",
    "                info = soup.find('div', {'id': 'newsct_article'}).text.strip()\n",
    "                # 기사 내용 데이터 분석을 위해 줄바꿈을 띄어쓰기로 변경\n",
    "                info = info.replace('\\n', ' ')\n",
    "                information.append(info)\n",
    "        except:\n",
    "            # 다른 구조에서 재 크롤링 코드\n",
    "            # 여기서 오류가 나는 경우는 게임/리뷰 기사가 아닌 다른 카테고리의 기사로 판단\n",
    "            try:\n",
    "                response = requests.get(main_link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                if response.status_code == 200:\n",
    "                    html = response.content\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    # 기사의 본문 내용을 담고 있는 부분\n",
    "                    info = soup.find('div', {'id': 'newsEndContents'}).text.strip()\n",
    "                    info = info.replace('\\n', ' ')\n",
    "                    # 해당 구조의 기사는 기자의 정보가 본문과 무조건 같이 존재\n",
    "                    # 기자의 정보 부분은 필요가 없기 때문에 기자 정보의 기준점이 되는 부분을 찾음\n",
    "                    # 기자의 정보 기준이 기재정보라는 단어이기 때문에 그 이후는 삭제\n",
    "                    '''\n",
    "                    end = info.index('기재정보')\n",
    "                    info = info[:end]\n",
    "                    '''\n",
    "                    information.append(info)\n",
    "            except Exception as e:\n",
    "                info = ''\n",
    "                information.append(info)\n",
    "                # 오류가 발생하는 이유와 발생하는 링크를 출력하여 오류를 확인하는 장치\n",
    "                # print(e)\n",
    "                # print(main_link)\n",
    "\n",
    "    Information['information'] = information\n",
    " \n",
    "    with pd.ExcelWriter(excel_name) as writer:\n",
    "        Information.to_excel(writer, sheet_name='결과값', index=False)\n",
    "        print(f'엑셀 파일 저장 완료: {excel_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
